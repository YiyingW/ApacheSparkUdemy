Word Counts in Spark

1. Loading the file into an RDD, in memory across nodes in Spark cluster
	each record in the RDD represents a line in the text file
2. from these lines, extract every word.
	flatMap(lambda x: x.split()).map(lambda x: (x, 1))
	each record is a word in the file and a count of 1
3. reduceByKey to compute the word counts

This is a classic example of the MapReduce programming model. 

MapReduce is a programming model invented by Google. 
Hadoop uses MapReduce for all it's computing tasks. 
Distributed computing can get very complicated. How to manage tasks across multiple
	nodes? What to do if a node goes down?
MapReduce abstracts the programmer from all these complications. 
	You just define 2 functions: map(), reduce()
	different from Spark's built in operations. 
	The rest is taken care of by Hadoop. 
	Key insight: 
		ANY data processing task can be parallelized, if you express it as
		<key, value> -> map() -> <key, value> -> reduce() -> <key, value>
		or a chain of such transformations
